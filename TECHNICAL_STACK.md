# ğŸš€ Technical Stack & Project Architecture

## **Technology Stack**

### **Backend**
- **Runtime**: Node.js (v16+)
- **Framework**: Express.js 4.18.2
- **Language**: JavaScript (ES6+)
- **Package Manager**: npm

### **Frontend**
- **Framework**: Vanilla JavaScript (React-style)
- **UI**: HTML5 + CSS3
- **HTTP Client**: Axios
- **Styling**: Custom CSS with dark theme

### **AI/LLM Integration**
- **Primary**: Ollama (Local LLM hosting)
  - Model: `gpt-oss:120b-cloud` (configurable)
  - API: REST API (http://localhost:11434)

### **Data Layer**
- **Format**: CSV files
  - `Sample_Grocery_Data.csv` (products)
  - `Sample_Recipes_Data.csv` (recipes)
- **Parser**: csv-parse 5.4.0

### **Key Dependencies**
```json
{
  "axios": "^1.12.2",        // HTTP requests to Ollama/OpenAI
  "cors": "^2.8.5",          // Cross-Origin Resource Sharing
  "csv-parse": "^5.4.0",     // CSV data parsing
  "express": "^4.18.2",      // Web server framework
  "dotenv": "^16.4.5",       // Environment variable management
  "openai": "^4.56.0"        // OpenAI SDK (optional)
}
```

### **Development Tools**
- **Testing**: Jest 29.6.1
- **Hot Reload**: Nodemon 2.0.22
- **Version Control**: Git + GitHub

---

## **ğŸ“‚ Project Structure & File Flow**

```
Updated Grocery Site/
â”‚
â”œâ”€â”€ backend/                          # Server-side application
â”‚   â”œâ”€â”€ index.js                      # â­ MAIN ENTRY POINT
â”‚   â”œâ”€â”€ package.json                  # Dependencies & scripts
â”‚   â”œâ”€â”€ .env                          # Environment config (NOT in Git)
â”‚   â”œâ”€â”€ .env.example                  # Template for .env
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                         # Data source
â”‚   â”‚   â”œâ”€â”€ Sample_Grocery_Data.csv   # Product catalog
â”‚   â”‚   â””â”€â”€ Sample_Recipes_Data.csv   # Recipe database
â”‚   â”‚
â”‚   â”œâ”€â”€ src/                          # Core business logic
â”‚   â”‚   â”œâ”€â”€ dataLoader.js             # CSV parsing & caching
â”‚   â”‚   â”œâ”€â”€ ollamaService.js          # LLM provider abstraction
â”‚   â”‚   â””â”€â”€ chatLogic.js              # Chat processing & recipe matching
â”‚   â”‚
â”‚   â”œâ”€â”€ public/                       # Frontend files (served as static)
â”‚   â”‚   â”œâ”€â”€ index.html                # UI entry point
â”‚   â”‚   â”œâ”€â”€ app.js                    # Frontend JavaScript
â”‚   â”‚   â””â”€â”€ styles.css                # Styling
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                        # Unit tests
â”‚   â”‚   â””â”€â”€ dataLoader.test.js
â”‚   â”‚
â”‚   â””â”€â”€ scripts/                      # Utility scripts
â”‚       â””â”€â”€ llm_probe.js              # LLM testing without server
â”‚
â”œâ”€â”€ OLLAMA_SETUP_GUIDE.md            # Ollama installation guide
â”œâ”€â”€ README.md                         # Project overview
â””â”€â”€ .gitignore                        # Git exclusion rules
```

---

## **ğŸ”„ Request Flow Diagram**

### **1. Application Startup**

```
User runs: npm start
    â†“
backend/index.js (Entry Point)
    â”œâ”€â”€ require('dotenv').config()               â†’ Load .env variables
    â”œâ”€â”€ require('./src/dataLoader')              â†’ Initialize data loader
    â”œâ”€â”€ require('./src/ollamaService')           â†’ Initialize LLM service
    â”œâ”€â”€ require('./src/chatLogic')               â†’ Initialize chat engine
    â†“
Express Server starts on port 3333
    â”œâ”€â”€ Serve static files from /public
    â”œâ”€â”€ Mount API routes:
    â”‚   â”œâ”€â”€ GET  /api/products                   â†’ Returns product catalog
    â”‚   â”œâ”€â”€ GET  /api/recipes                    â†’ Returns recipe database
    â”‚   â”œâ”€â”€ POST /api/chat                       â†’ Processes user messages
    â”‚   â””â”€â”€ GET  /api/llm/health                 â†’ LLM health check
    â†“
Server listening on http://127.0.0.1:3333
```

---

### **2. User Opens Browser**

```
User navigates to: http://127.0.0.1:3333
    â†“
Express serves: backend/public/index.html
    â†“
Browser loads:
    â”œâ”€â”€ public/styles.css                        â†’ UI styling
    â””â”€â”€ public/app.js                            â†’ Frontend logic
    â†“
Frontend initialization:
    â”œâ”€â”€ Fetch GET /api/products                  â†’ Load product list
    â”œâ”€â”€ Fetch GET /api/recipes                   â†’ Load recipe list
    â””â”€â”€ Display welcome message
```

---

### **3. User Sends Chat Message**

```
User types: "give me 3 protein rich recipes"
    â†“
Frontend (app.js)
    â”œâ”€â”€ captureMessage()                         â†’ Get user input
    â”œâ”€â”€ displayMessage(user, message)            â†’ Show user message
    â””â”€â”€ fetch POST /api/chat { message }         â†’ Send to backend
    â†“
Backend (index.js) receives POST /api/chat
    â†“
Route handler calls: processMessage(message, context, loadedData)
    â†“
chatLogic.js â†’ processMessage()
    â”œâ”€â”€ 1. Parse user intent
    â”‚   â”œâ”€â”€ isGreeting? â†’ Return friendly greeting
    â”‚   â”œâ”€â”€ isShoppingList? â†’ Return list items
    â”‚   â”œâ”€â”€ needsRecipeSuggestion? â†’ Continue
    â”‚   â””â”€â”€ Extract recipe count: "3"
    â”‚
    â”œâ”€â”€ 2. Check if themed request
    â”‚   â””â”€â”€ isThemedRequest? â†’ Skip dataset, use LLM only
    â”‚
    â”œâ”€â”€ 3. Check if formatting request
    â”‚   â””â”€â”€ isFormattingRequest? â†’ Extract previous recipe names
    â”‚
    â”œâ”€â”€ 4. Call LLM for recipe generation
    â”‚   â†“
    â”‚   ollamaService.js â†’ suggestWithOllama()
    â”‚       â”œâ”€â”€ Build system prompt with JSON schema
    â”‚       â”œâ”€â”€ Add conversation context
    â”‚       â”œâ”€â”€ POST http://localhost:11434/api/chat
    â”‚       â”‚   {
    â”‚       â”‚     model: "gpt-oss:120b-cloud",
    â”‚       â”‚     messages: [...],
    â”‚       â”‚     format: "json",
    â”‚       â”‚     options: { num_predict: 2000 }
    â”‚       â”‚   }
    â”‚       â†“
    â”‚       Ollama LLM generates:
    â”‚       {
    â”‚         "reply": "Here are 3 protein-rich recipes!",
    â”‚         "reasoning": "These recipes are high in protein...",
    â”‚         "recipes": [
    â”‚           {
    â”‚             "name": "Protein Smoothie",
    â”‚             "ingredients": ["..."],
    â”‚             "steps": ["..."],
    â”‚             "mealType": "breakfast",
    â”‚             "autogenerated": true
    â”‚           },
    â”‚           // ... 2 more recipes
    â”‚         ]
    â”‚       }
    â”‚       â†“
    â”‚   Return parsed JSON
    â”‚
    â”œâ”€â”€ 5. Process LLM response
    â”‚   â”œâ”€â”€ Extract recipes array
    â”‚   â”œâ”€â”€ Normalize ingredients/steps
    â”‚   â”œâ”€â”€ Enrich with product availability
    â”‚   â””â”€â”€ Build final reply with reasoning
    â”‚
    â”œâ”€â”€ 6. Update context
    â”‚   â”œâ”€â”€ Add recipe names to seenRecipes
    â”‚   â”œâ”€â”€ Store in allSuggestedRecipes
    â”‚   â””â”€â”€ Update conversation history
    â”‚
    â””â”€â”€ 7. Return response
        {
          reply: "Here are 3 protein-rich recipes!\n\nğŸ’¡ These recipes...",
          recipes: [...],
          context: {...}
        }
    â†“
Backend sends JSON response to frontend
    â†“
Frontend (app.js) receives response
    â”œâ”€â”€ displayMessage(bot, reply)               â†’ Show text response
    â””â”€â”€ displayRecipeCards(recipes)              â†’ Render recipe cards
        â”œâ”€â”€ For each recipe:
        â”‚   â”œâ”€â”€ Show name, meal type
        â”‚   â”œâ”€â”€ List ingredients (check availability)
        â”‚   â”œâ”€â”€ "Add Ingredients" button
        â”‚   â””â”€â”€ "More" button (expand for steps)
        â””â”€â”€ Update shopping list if needed
```

---

## **ğŸ§© Key File Responsibilities**

### **backend/index.js** (Main Entry Point)
**Purpose**: Express server initialization and route definitions

**Key Functions**:
- `require('dotenv').config()` - Load environment variables
- `app.use(cors())` - Enable cross-origin requests
- `app.use(express.static('public'))` - Serve frontend files
- `app.post('/api/chat')` - Main chat endpoint
- `app.get('/api/products')` - Product catalog endpoint
- `app.get('/api/recipes')` - Recipe database endpoint
- `app.listen(PORT)` - Start HTTP server

**Flow**:
```javascript
1. Load environment config (.env)
2. Initialize Express app
3. Configure middleware (CORS, JSON parser)
4. Load CSV data via dataLoader
5. Mount API routes
6. Start server on port 3333
```

---

### **backend/src/dataLoader.js** (Data Management)
**Purpose**: Parse CSV files and cache data in memory

**Key Functions**:
- `loadProducts()` - Parse `Sample_Grocery_Data.csv`
- `loadRecipes()` - Parse `Sample_Recipes_Data.csv`
- `loadAllData()` - Load both datasets and cache

**Data Structures**:
```javascript
products: [
  { item: "Mozzarella Cheese", category: "Dairy", price: "$9.99" }
]

recipes: [
  { 
    name: "Chicken Stir Fry",
    ingredients: ["chicken", "soy sauce", ...],
    steps: "Heat oil...",
    mealType: "dinner"
  }
]
```

**Flow**:
```javascript
1. Read CSV files from /data folder
2. Parse using csv-parse library
3. Normalize and structure data
4. Cache in memory for fast access
5. Return to requesting module
```

---

### **backend/src/ollamaService.js** (LLM Provider)
**Purpose**: Abstract LLM interactions (Ollama/OpenAI)

**Key Functions**:
- `chatWithOllama(message, context)` - Natural language responses
- `suggestWithOllama({ message, requestedCount })` - Structured recipe generation
- `chatWithOpenAI()` - OpenAI alternative
- `suggestWithOpenAI()` - OpenAI recipe generation

**Provider Selection Logic**:
```javascript
const PROVIDER = process.env.LLM_PROVIDER || 
                 (hasValidOpenAIKey ? 'openai' : 'ollama');

if (PROVIDER === 'openai') {
  exports.chatWithLLM = chatWithOpenAI;
  exports.suggest = suggestWithOpenAI;
} else {
  exports.chatWithLLM = chatWithOllama;
  exports.suggest = suggestWithOllama;
}
```

**LLM Request Structure**:
```javascript
POST http://localhost:11434/api/chat
{
  "model": "gpt-oss:120b-cloud",
  "messages": [
    { "role": "system", "content": "You are a recipe assistant..." },
    { "role": "user", "content": "give me 3 protein rich recipes" }
  ],
  "format": "json",
  "options": { 
    "temperature": 0.5,
    "num_predict": 2000  // Max tokens to generate
  }
}
```

**Response Handling**:
```javascript
1. Receive JSON from Ollama
2. Validate JSON structure
3. If malformed:
   - Attempt JSON repair (remove trailing commas)
   - Extract partial recipes using regex
   - Return fallback response
4. If valid:
   - Parse recipes array
   - Extract reasoning
   - Return structured data
```

---

### **backend/src/chatLogic.js** (Chat Engine)
**Purpose**: Process user messages and orchestrate responses

**Key Functions**:
- `processMessage(message, context, data)` - Main chat handler
- `findRecipesForOccasion()` - Dataset-based recipe matching
- `enrichRecipesWithProducts()` - Add product availability
- `chooseBestRecipe()` - Select optimal recipe from candidates

**Decision Flow**:
```javascript
1. Intent Recognition
   â”œâ”€â”€ Greeting? â†’ "Hi! I can help with recipes..."
   â”œâ”€â”€ Shopping list query? â†’ Return list items
   â”œâ”€â”€ "More" request? â†’ Generate additional recipes
   â””â”€â”€ Recipe request? â†’ Continue processing

2. Request Type Detection
   â”œâ”€â”€ isThemedRequest? (halloween, christmas)
   â”‚   â””â”€â”€ Force LLM generation, skip dataset
   â”œâ”€â”€ isFormattingRequest? ("give me the recipe of these")
   â”‚   â””â”€â”€ Extract previous recipe names, format as cards
   â””â”€â”€ Regular request? â†’ Continue

3. Recipe Count Extraction
   â”œâ”€â”€ Regex: /\b(\d+|one|two|three...ten)\b/i
   â”œâ”€â”€ Map number words: "three" â†’ 3
   â””â”€â”€ Default: 3 recipes

4. LLM Generation
   â”œâ”€â”€ Build prompt with:
   â”‚   â”œâ”€â”€ User query
   â”‚   â”œâ”€â”€ Recipe count requirement
   â”‚   â”œâ”€â”€ Conversation context
   â”‚   â”œâ”€â”€ Avoid list (seen recipes)
   â”‚   â””â”€â”€ Grounded mode (if requested)
   â””â”€â”€ Call suggestWithOllama({ requestedCount })

5. Response Processing
   â”œâ”€â”€ Parse LLM JSON response
   â”œâ”€â”€ Extract: reply, reasoning, recipes[]
   â”œâ”€â”€ Normalize recipe structure
   â”œâ”€â”€ Enrich with product data
   â””â”€â”€ Update conversation context

6. Return Format
   {
     reply: "text with ğŸ’¡ reasoning",
     recipes: [{ name, ingredients, steps, mealType }],
     context: { messages, seenRecipes, shoppingList }
   }
```

**Context Management**:
```javascript
context = {
  messages: [
    { from: 'user', text: '...' },
    { from: 'bot', text: '...' }
  ],
  seenRecipes: Set(['Recipe1', 'Recipe2']),
  allSuggestedRecipes: [...],
  shoppingList: [],
  lastNonMoreQuery: "..."
}
```

---

### **backend/public/app.js** (Frontend Logic)
**Purpose**: Handle UI interactions and API communication

**Key Functions**:
- `loadInitialData()` - Fetch products and recipes on page load
- `sendMessage()` - POST user message to /api/chat
- `displayMessage()` - Render text messages in chat
- `displayRecipeCards()` - Render recipe cards with ingredients
- `addIngredientsToCart()` - Add recipe items to shopping list

**Event Flow**:
```javascript
User clicks "Send" button
    â†“
captureMessage()
    â”œâ”€â”€ Get input value
    â”œâ”€â”€ Validate non-empty
    â””â”€â”€ Call sendMessage(message)
    â†“
sendMessage(message)
    â”œâ”€â”€ Display user message in chat
    â”œâ”€â”€ Show "typing..." indicator
    â”œâ”€â”€ POST /api/chat { message, context }
    â”œâ”€â”€ Receive JSON response
    â”œâ”€â”€ Hide typing indicator
    â”œâ”€â”€ displayMessage(bot, response.reply)
    â””â”€â”€ displayRecipeCards(response.recipes)
    â†“
displayRecipeCards(recipes)
    â”œâ”€â”€ For each recipe:
    â”‚   â”œâ”€â”€ Create card HTML
    â”‚   â”œâ”€â”€ Map ingredients to products
    â”‚   â”œâ”€â”€ Check availability (in stock?)
    â”‚   â”œâ”€â”€ Add "Add Ingredients" button
    â”‚   â””â”€â”€ Add "More" button (expand steps)
    â””â”€â”€ Append to chat container
```

---

## **âš™ï¸ Environment Configuration**

### **backend/.env**
```bash
# LLM Provider Selection
LLM_PROVIDER=ollama                    # 'ollama' or 'openai'

# Ollama Configuration
OLLAMA_URL=http://localhost:11434      # Ollama API endpoint
OLLAMA_MODEL=gpt-oss:120b-cloud        # Model name (customizable)
OLLAMA_API_KEY=                        # Optional API key for cloud

# OpenAI Configuration (Alternative)
OPENAI_API_KEY=                        # Your OpenAI API key
OPENAI_MODEL=gpt-4o-mini               # OpenAI model

# Server Configuration
PORT=3333                              # Backend server port
HOST=127.0.0.1                         # Bind address
DEBUG=0                                # Debug logging
```

---

## **ğŸ” Special Features**

### **1. Themed Recipe Generation**
**Trigger**: Keywords like "halloween", "christmas", "romantic"
**Behavior**: Skip dataset fallback, force creative LLM generation
**Code**: `chatLogic.js` line 1367-1368

### **2. Recipe Count Honoring**
**Trigger**: "give me 3 recipes", "two breakfast ideas"
**Behavior**: Extract number, pass to LLM as `requestedCount`
**Code**: `chatLogic.js` line 1364-1377

### **3. Formatting Requests**
**Trigger**: "give me the recipe of these", "format previous recipes"
**Behavior**: Extract recipe names from last bot message, return as cards
**Code**: `chatLogic.js` line 1237-1333

### **4. Reasoning Display**
**Trigger**: Any recipe suggestion
**Behavior**: LLM explains why recipes were chosen
**Format**: `ğŸ’¡ These recipes are high in protein and quick to prepare.`
**Code**: `chatLogic.js` line 1491-1494

### **5. Product Availability**
**Behavior**: Check if recipe ingredients exist in product catalog
**Display**: Green "(in stock)" or red "(not found)"
**Code**: `chatLogic.js` enrichRecipesWithProducts()

---

## **ğŸ§ª Testing**

### **Run Tests**
```bash
cd backend
npm test
```

### **Test Files**
- `tests/dataLoader.test.js` - Validates CSV parsing

### **Manual Testing**
```bash
# Test LLM without server
node backend/scripts/llm_probe.js

# Health check
curl http://127.0.0.1:3333/api/llm/health?probe=1
```

---

## **ğŸ“Š Data Flow Summary**

```
CSV Files â†’ dataLoader.js â†’ Cached in Memory
                                â†“
User Input â†’ Frontend (app.js) â†’ POST /api/chat
                                â†“
Backend (index.js) â†’ chatLogic.js â†’ Process Intent
                                â†“
                    ollamaService.js â†’ Call Ollama LLM
                                â†“
                    Ollama generates JSON recipes
                                â†“
chatLogic.js â† Parse & Enrich â† Add product data
                                â†“
Backend sends JSON response â†’ Frontend displays cards
```

---

## **ğŸš€ Deployment Checklist**

1. âœ… Install Node.js 16+
2. âœ… Install Ollama + pull model (`ollama pull gpt-oss:120b-cloud`)
3. âœ… Clone repository
4. âœ… Run `npm install` in backend/
5. âœ… Create `.env` file from `.env.example`
6. âœ… Configure `OLLAMA_MODEL` in `.env`
7. âœ… Start server: `npm start`
8. âœ… Open browser: http://127.0.0.1:3333

---

## **ğŸ“ Quick Reference**

| Component | File | Port/URL |
|-----------|------|----------|
| Backend Server | `backend/index.js` | 3333 |
| Frontend UI | `backend/public/index.html` | http://127.0.0.1:3333 |
| Ollama API | External service | 11434 |
| Product Data | `backend/data/Sample_Grocery_Data.csv` | - |
| Recipe Data | `backend/data/Sample_Recipes_Data.csv` | - |

---

**Last Updated**: October 31, 2025
**Version**: 1.0.0
**Maintainer**: Vedantshi
